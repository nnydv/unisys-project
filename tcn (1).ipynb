{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKq406CycT8J",
        "outputId": "c98ce9aa-4b64-4352-b98a-bc91cfab942e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X17hsxp31UAI",
        "outputId": "13a1f4b4-e83d-4f80-8b02-e4d19f675967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMpwlorX0wnf",
        "outputId": "c5a764d5-30c4-4bf3-a41c-6aba9ae4160b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Fashion/predict-energy-behavior-of-prosumers.zip\n",
            "  inflating: /content/dataset/client.csv  \n",
            "  inflating: /content/dataset/county_id_to_name_map.json  \n",
            "  inflating: /content/dataset/electricity_prices.csv  \n",
            "  inflating: /content/dataset/enefit/__init__.py  \n",
            "  inflating: /content/dataset/enefit/competition.cpython-310-x86_64-linux-gnu.so  \n",
            "  inflating: /content/dataset/example_test_files/client.csv  \n",
            "  inflating: /content/dataset/example_test_files/electricity_prices.csv  \n",
            "  inflating: /content/dataset/example_test_files/forecast_weather.csv  \n",
            "  inflating: /content/dataset/example_test_files/gas_prices.csv  \n",
            "  inflating: /content/dataset/example_test_files/historical_weather.csv  \n",
            "  inflating: /content/dataset/example_test_files/revealed_targets.csv  \n",
            "  inflating: /content/dataset/example_test_files/sample_submission.csv  \n",
            "  inflating: /content/dataset/example_test_files/test.csv  \n",
            "  inflating: /content/dataset/forecast_weather.csv  \n",
            "  inflating: /content/dataset/gas_prices.csv  \n",
            "  inflating: /content/dataset/historical_weather.csv  \n",
            "  inflating: /content/dataset/public_timeseries_testing_util.py  \n",
            "  inflating: /content/dataset/train.csv  \n",
            "  inflating: /content/dataset/weather_station_to_county_mapping.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Fashion/predict-energy-behavior-of-prosumers.zip -d /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAz7eOIwfcfg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2s20U1Ofcfl"
      },
      "outputs": [],
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        X = X.reset_index()\n",
        "\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            idx_train = X[dates.values < t].index\n",
        "            idx_test = X[dates.values == t].index\n",
        "\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        return self.n_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGKUSz79fcfo"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
        "    # Convert datetime column to date\n",
        "    df_data = (\n",
        "        df_data.with_columns(\n",
        "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Adjust client data dates\n",
        "    df_client = (\n",
        "        df_client.with_columns(\n",
        "            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Adjust gas data dates and rename columns\n",
        "    df_gas = (\n",
        "        df_gas.rename({\"forecast_date\": \"date\"}).with_columns(\n",
        "            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Adjust electricity data dates and rename columns\n",
        "    df_electricity = (\n",
        "        df_electricity.rename({\"forecast_date\": \"datetime\"}).with_columns(\n",
        "            pl.col(\"datetime\") + pl.duration(days=1)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Convert latitude and longitude to float in location data\n",
        "    df_location = (\n",
        "        df_location.with_columns(\n",
        "            pl.col(\"latitude\").cast(pl.Float32),\n",
        "            pl.col(\"longitude\").cast(pl.Float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Adjust forecast data, join with location, and drop unnecessary columns\n",
        "    df_forecast = (\n",
        "        df_forecast.rename({\"forecast_datetime\": \"datetime\"}).with_columns(\n",
        "            pl.col(\"latitude\").cast(pl.Float32),\n",
        "            pl.col(\"longitude\").cast(pl.Float32),\n",
        "        )\n",
        "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
        "        .drop([\"longitude\", \"latitude\"])\n",
        "    )\n",
        "\n",
        "    # Adjust historical data, join with location, and drop unnecessary columns\n",
        "    df_historical = (\n",
        "        df_historical.with_columns(\n",
        "            pl.col(\"latitude\").cast(pl.Float32),\n",
        "            pl.col(\"longitude\").cast(pl.Float32),\n",
        "            pl.col(\"datetime\") + pl.duration(hours=37)\n",
        "        )\n",
        "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
        "        .drop([\"longitude\", \"latitude\"])\n",
        "    )\n",
        "\n",
        "    # Aggregate forecast and historical data\n",
        "    df_forecast_date = df_forecast.group_by(\"datetime\").mean().drop(\"county\")\n",
        "    df_forecast_local = df_forecast.filter(pl.col(\"county\").is_not_null()).group_by([\"county\", \"datetime\"]).mean()\n",
        "    df_historical_date = df_historical.group_by(\"datetime\").mean().drop(\"county\")\n",
        "    df_historical_local = df_historical.filter(pl.col(\"county\").is_not_null()).group_by([\"county\", \"datetime\"]).mean()\n",
        "\n",
        "    # Join all dataframes together on appropriate columns\n",
        "    df_data = (\n",
        "        df_data\n",
        "        .join(df_gas, on=\"date\", how=\"left\")\n",
        "        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
        "        .join(df_electricity, on=\"datetime\", how=\"left\")\n",
        "        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
        "        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
        "        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n",
        "        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n",
        "\n",
        "        # Join with forecast and historical data shifted by 7 days\n",
        "        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fdw\")\n",
        "        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_flw\")\n",
        "        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hdw\")\n",
        "        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hlw\")\n",
        "\n",
        "        # Join target data shifted by various days\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
        "\n",
        "        # Create date-related features\n",
        "        .with_columns(\n",
        "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
        "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
        "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
        "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
        "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
        "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
        "        )\n",
        "\n",
        "        # Create category column\n",
        "        .with_columns(\n",
        "            pl.concat_str([\"county\", \"is_business\", \"product_type\", \"is_consumption\"], separator=\"_\").alias(\"category_1\"),\n",
        "        )\n",
        "\n",
        "        # Create cyclical features for time of year and time of day\n",
        "        .with_columns(\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
        "        )\n",
        "\n",
        "        # Convert float64 columns to float32 for memory efficiency\n",
        "        .with_columns(\n",
        "            pl.col(pl.Float64).cast(pl.Float32),\n",
        "        )\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        .drop([\"date\", \"datetime\", \"hour\", \"dayofyear\"])\n",
        "    )\n",
        "\n",
        "    return df_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf8vpwPlfcfq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def to_pandas(X, y=None):\n",
        "    # List of categorical columns\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    # Convert Polars DataFrame to Pandas DataFrame\n",
        "    if y is not None:\n",
        "        # Concatenate features and target\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    # Set row_id as index\n",
        "    df = df.set_index(\"row_id\")\n",
        "\n",
        "    # Convert categorical columns to category dtype\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    # Calculate the mean of target columns\n",
        "    target_cols = [f\"target_{i}\" for i in range(1, 7)]\n",
        "    df[\"target_mean\"] = df[target_cols].mean(axis=1)\n",
        "\n",
        "    # Calculate the standard deviation of target columns\n",
        "    df[\"target_std\"] = df[target_cols].std(axis=1)\n",
        "\n",
        "    # Calculate the ratio of target_6 to target_7\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)  # Added a small value to avoid division by zero\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzDZLH7fcfs"
      },
      "outputs": [],
      "source": [
        "def lgb_objective(trial):\n",
        "    params = {\n",
        "        'n_iter'           : 1000,\n",
        "        'verbose'          : -1,\n",
        "        'random_state'     : 42,\n",
        "        'objective'        : 'l2',\n",
        "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n",
        "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n",
        "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),\n",
        "        'max_depth'        : trial.suggest_int('max_depth', 5, 10),\n",
        "        'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n",
        "    }\n",
        "\n",
        "    model  = lgb.LGBMRegressor(**params)\n",
        "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "    cv     = MonthlyKFold(1)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    return -1 * np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEEvajajfcfu",
        "outputId": "2c3ca4cd-6698-4506-af42-51ff97ca8e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1651902 entries, 366048 to 2018351\n",
            "Data columns (total 137 columns):\n",
            " #    Column                                 Dtype   \n",
            "---   ------                                 -----   \n",
            " 0    county                                 category\n",
            " 1    is_business                            category\n",
            " 2    product_type                           category\n",
            " 3    is_consumption                         category\n",
            " 4    lowest_price_per_mwh                   float32 \n",
            " 5    highest_price_per_mwh                  float32 \n",
            " 6    eic_count                              float64 \n",
            " 7    installed_capacity                     float32 \n",
            " 8    euros_per_mwh                          float32 \n",
            " 9    hours_ahead                            float32 \n",
            " 10   temperature                            float32 \n",
            " 11   dewpoint                               float32 \n",
            " 12   cloudcover_high                        float32 \n",
            " 13   cloudcover_low                         float32 \n",
            " 14   cloudcover_mid                         float32 \n",
            " 15   cloudcover_total                       float32 \n",
            " 16   10_metre_u_wind_component              float32 \n",
            " 17   10_metre_v_wind_component              float32 \n",
            " 18   direct_solar_radiation                 float32 \n",
            " 19   surface_solar_radiation_downwards      float32 \n",
            " 20   snowfall                               float32 \n",
            " 21   total_precipitation                    float32 \n",
            " 22   hours_ahead_fl                         float32 \n",
            " 23   temperature_fl                         float32 \n",
            " 24   dewpoint_fl                            float32 \n",
            " 25   cloudcover_high_fl                     float32 \n",
            " 26   cloudcover_low_fl                      float32 \n",
            " 27   cloudcover_mid_fl                      float32 \n",
            " 28   cloudcover_total_fl                    float32 \n",
            " 29   10_metre_u_wind_component_fl           float32 \n",
            " 30   10_metre_v_wind_component_fl           float32 \n",
            " 31   direct_solar_radiation_fl              float32 \n",
            " 32   surface_solar_radiation_downwards_fl   float32 \n",
            " 33   snowfall_fl                            float32 \n",
            " 34   total_precipitation_fl                 float32 \n",
            " 35   temperature_hd                         float32 \n",
            " 36   dewpoint_hd                            float32 \n",
            " 37   rain                                   float32 \n",
            " 38   snowfall_hd                            float32 \n",
            " 39   surface_pressure                       float32 \n",
            " 40   cloudcover_total_hd                    float32 \n",
            " 41   cloudcover_low_hd                      float32 \n",
            " 42   cloudcover_mid_hd                      float32 \n",
            " 43   cloudcover_high_hd                     float32 \n",
            " 44   windspeed_10m                          float32 \n",
            " 45   winddirection_10m                      float32 \n",
            " 46   shortwave_radiation                    float32 \n",
            " 47   direct_solar_radiation_hd              float32 \n",
            " 48   diffuse_radiation                      float32 \n",
            " 49   temperature_hl                         float32 \n",
            " 50   dewpoint_hl                            float32 \n",
            " 51   rain_hl                                float32 \n",
            " 52   snowfall_hl                            float32 \n",
            " 53   surface_pressure_hl                    float32 \n",
            " 54   cloudcover_total_hl                    float32 \n",
            " 55   cloudcover_low_hl                      float32 \n",
            " 56   cloudcover_mid_hl                      float32 \n",
            " 57   cloudcover_high_hl                     float32 \n",
            " 58   windspeed_10m_hl                       float32 \n",
            " 59   winddirection_10m_hl                   float32 \n",
            " 60   shortwave_radiation_hl                 float32 \n",
            " 61   direct_solar_radiation_hl              float32 \n",
            " 62   diffuse_radiation_hl                   float32 \n",
            " 63   hours_ahead_fdw                        float32 \n",
            " 64   temperature_fdw                        float32 \n",
            " 65   dewpoint_fdw                           float32 \n",
            " 66   cloudcover_high_fdw                    float32 \n",
            " 67   cloudcover_low_fdw                     float32 \n",
            " 68   cloudcover_mid_fdw                     float32 \n",
            " 69   cloudcover_total_fdw                   float32 \n",
            " 70   10_metre_u_wind_component_fdw          float32 \n",
            " 71   10_metre_v_wind_component_fdw          float32 \n",
            " 72   direct_solar_radiation_fdw             float32 \n",
            " 73   surface_solar_radiation_downwards_fdw  float32 \n",
            " 74   snowfall_fdw                           float32 \n",
            " 75   total_precipitation_fdw                float32 \n",
            " 76   hours_ahead_flw                        float32 \n",
            " 77   temperature_flw                        float32 \n",
            " 78   dewpoint_flw                           float32 \n",
            " 79   cloudcover_high_flw                    float32 \n",
            " 80   cloudcover_low_flw                     float32 \n",
            " 81   cloudcover_mid_flw                     float32 \n",
            " 82   cloudcover_total_flw                   float32 \n",
            " 83   10_metre_u_wind_component_flw          float32 \n",
            " 84   10_metre_v_wind_component_flw          float32 \n",
            " 85   direct_solar_radiation_flw             float32 \n",
            " 86   surface_solar_radiation_downwards_flw  float32 \n",
            " 87   snowfall_flw                           float32 \n",
            " 88   total_precipitation_flw                float32 \n",
            " 89   temperature_hdw                        float32 \n",
            " 90   dewpoint_hdw                           float32 \n",
            " 91   rain_hdw                               float32 \n",
            " 92   snowfall_hdw                           float32 \n",
            " 93   surface_pressure_hdw                   float32 \n",
            " 94   cloudcover_total_hdw                   float32 \n",
            " 95   cloudcover_low_hdw                     float32 \n",
            " 96   cloudcover_mid_hdw                     float32 \n",
            " 97   cloudcover_high_hdw                    float32 \n",
            " 98   windspeed_10m_hdw                      float32 \n",
            " 99   winddirection_10m_hdw                  float32 \n",
            " 100  shortwave_radiation_hdw                float32 \n",
            " 101  direct_solar_radiation_hdw             float32 \n",
            " 102  diffuse_radiation_hdw                  float32 \n",
            " 103  temperature_hlw                        float32 \n",
            " 104  dewpoint_hlw                           float32 \n",
            " 105  rain_hlw                               float32 \n",
            " 106  snowfall_hlw                           float32 \n",
            " 107  surface_pressure_hlw                   float32 \n",
            " 108  cloudcover_total_hlw                   float32 \n",
            " 109  cloudcover_low_hlw                     float32 \n",
            " 110  cloudcover_mid_hlw                     float32 \n",
            " 111  cloudcover_high_hlw                    float32 \n",
            " 112  windspeed_10m_hlw                      float32 \n",
            " 113  winddirection_10m_hlw                  float32 \n",
            " 114  shortwave_radiation_hlw                float32 \n",
            " 115  direct_solar_radiation_hlw             float32 \n",
            " 116  diffuse_radiation_hlw                  float32 \n",
            " 117  target_1                               float32 \n",
            " 118  target_2                               float32 \n",
            " 119  target_3                               float32 \n",
            " 120  target_4                               float32 \n",
            " 121  target_5                               float32 \n",
            " 122  target_6                               float32 \n",
            " 123  target_7                               float32 \n",
            " 124  day                                    int8    \n",
            " 125  weekday                                int8    \n",
            " 126  month                                  int8    \n",
            " 127  year                                   int32   \n",
            " 128  category_1                             category\n",
            " 129  sin(dayofyear)                         float32 \n",
            " 130  cos(dayofyear)                         float32 \n",
            " 131  sin(hour)                              float32 \n",
            " 132  cos(hour)                              float32 \n",
            " 133  target                                 float64 \n",
            " 134  target_mean                            float32 \n",
            " 135  target_std                             float32 \n",
            " 136  target_ratio                           float32 \n",
            "dtypes: category(5), float32(126), float64(2), int32(1), int8(3)\n",
            "memory usage: 852.3 MB\n"
          ]
        }
      ],
      "source": [
        "root = \"/content/dataset\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = '/content/dataset/save'\n",
        "load_path = None\n",
        "#Data I/O\n",
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema\n",
        "#Feature Engineering\n",
        "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
        "\n",
        "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "df_train = to_pandas(X, y)\n",
        "df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]\n",
        "df_train.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX457iw_17b1",
        "outputId": "92bf405d-274e-40b2-eb0e-aa1fdabd725e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit Time(s): 855.654\n",
            "Score Time(s): 7.675\n",
            "Error(MAE): 62.798\n"
          ]
        }
      ],
      "source": [
        "#Global Variables\n",
        "# HyperParam Optimization\n",
        "# study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
        "# study.optimize(lgb_objective, n_trials=100, show_progress_bar=True)\n",
        "best_params = {\n",
        "     'n_iter'           : 900,\n",
        "     'verbose'          : -1,\n",
        "     'objective'        : 'l2',\n",
        "     'learning_rate'    : 0.05689066836106983,\n",
        "     'colsample_bytree' : 0.8915976762048253,\n",
        "     'colsample_bynode' : 0.5942203285139224,\n",
        "     'lambda_l1'        : 3.6277555139102864,\n",
        "     'lambda_l2'        : 1.6591278779517808,\n",
        "     'min_data_in_leaf' : 186,\n",
        "     'max_depth'        : 9,\n",
        "     'max_bin'          : 813,\n",
        " } # val score is 62.24 for the last month\n",
        "# Validation\n",
        "result = cross_validate(\n",
        "    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n",
        "    X=df_train.drop(columns=[\"target\"]),\n",
        "    y=df_train[\"target\"],\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=MonthlyKFold(1),\n",
        ")\n",
        "\n",
        "print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n",
        "print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n",
        "print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqi_jGzn26Om",
        "outputId": "bea4772e-55b2-4836-d6d7-bdf5026895f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "if load_path is not None:\n",
        "     model = pickle.load(open(load_path, \"rb\"))\n",
        "else:\n",
        "     model = VotingRegressor([\n",
        "         ('lgb_1', lgb.LGBMRegressor(**best_params, random_state=100)),\n",
        "         ('lgb_2', lgb.LGBMRegressor(**best_params, random_state=101)),\n",
        "         ('lgb_3', lgb.LGBMRegressor(**best_params, random_state=102)),\n",
        "         ('lgb_4', lgb.LGBMRegressor(**best_params, random_state=103)),\n",
        "         ('lgb_5', lgb.LGBMRegressor(**best_params, random_state=104)),\n",
        "     ])\n",
        "\n",
        "     model.fit(X=df_train.drop(columns=[\"target\"]), y=df_train[\"target\"])\n",
        "\n",
        "     # Save the model if a path is provided\n",
        "     if save_path is not None:\n",
        "         with open(save_path, \"wb\") as f:\n",
        "             pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8j5ikwtfcf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8ec116c5-842e-4d79-e907-aeda3d71561a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'enefit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b35053425ba0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menefit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miter_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m for (test, revealed_targets, client, historical_weather,\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enefit'"
          ]
        }
      ],
      "source": [
        "import enefit\n",
        "\n",
        "env = enefit.make_env()\n",
        "iter_test = env.iter_test()\n",
        "for (test, revealed_targets, client, historical_weather,\n",
        "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
        "\n",
        "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
        "\n",
        "    df_test           = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
        "    df_client         = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
        "    df_gas            = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
        "    df_electricity    = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
        "    df_new_forecast   = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
        "    df_new_historical = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
        "    df_new_target     = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
        "\n",
        "    df_forecast       = pl.concat([df_forecast, df_new_forecast]).unique()\n",
        "    df_historical     = pl.concat([df_historical, df_new_historical]).unique()\n",
        "    df_target         = pl.concat([df_target, df_new_target]).unique()\n",
        "\n",
        "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "    X_test = to_pandas(X_test)\n",
        "\n",
        "    # Predict using the MEMOTCN models\n",
        "    sample_prediction[\"target\"] = np.zeros_like(sample_prediction[\"target\"])\n",
        "    for i, row in X_test.iterrows():\n",
        "        category = (row['county'], row['is_business'], row['product_type'], row['is_consumption'])\n",
        "        if category in models:\n",
        "            # Prepare the test data for the MEMOTCN model\n",
        "            test_data = row.drop(columns=['county', 'is_business', 'product_type', 'is_consumption', 'category_1']).values.reshape(1, seq_length, -1)\n",
        "            prediction = models[category].predict(test_data)[0, 0]\n",
        "            sample_prediction[\"target\"][i] = prediction\n",
        "\n",
        "    env.predict(sample_prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}